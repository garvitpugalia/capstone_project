{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "LvikUVyWjJtb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u46MEtaEiuKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65185685-7730-4eff-eace-25e59cabb487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/302.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (7.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtYx3qbttze_",
        "outputId": "711d1f4d-02af-4b0e-c55a-cdc2470fac9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, ControlNetModel, AutoencoderKL, PNDMScheduler\n",
        "from PIL import Image\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import os"
      ],
      "metadata": {
        "id": "dWLSkZKytw-O"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "OHyWLSYgcQB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageShadowDepthLightingDataset(Dataset):\n",
        "    def __init__(self, image_dir, shadow_dir, depth_dir, lighting_directions=None, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.shadow_dir = shadow_dir\n",
        "        self.depth_dir = depth_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.shadow_files = sorted(os.listdir(shadow_dir))\n",
        "        self.depth_files = sorted(os.listdir(depth_dir))\n",
        "        if lighting_directions != None:\n",
        "          self.lighting_coords = self.load_lighting_coords(lighting_directions)\n",
        "\n",
        "    def load_lighting_coords(self, lighting_file):\n",
        "        with open(lighting_file, 'r') as f:\n",
        "            coords = [list(map(float, line.strip().split())) for line in f]\n",
        "        return coords\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.image_dir, self.image_files[idx])).convert('RGB')\n",
        "        shadow = Image.open(os.path.join(self.shadow_dir, self.shadow_files[idx])).convert('RGB')\n",
        "        depth = Image.open(os.path.join(self.depth_dir, self.depth_files[idx]))\n",
        "        # lighting = torch.tensor(self.lighting_coords[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            shadow = self.transform(shadow)\n",
        "            depth = self.transform(depth)\n",
        "\n",
        "        return image, shadow, depth"
      ],
      "metadata": {
        "id": "0AWAE9IxtmFl"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update to break image into pieces of 256x256, instead of resizing for more data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = ImageShadowDepthLightingDataset(\n",
        "    image_dir=\"/content/drive/MyDrive/Capstone/SOBA-high-res/images/\",\n",
        "    shadow_dir=\"/content/drive/MyDrive/Capstone/SOBA-high-res/shadow_maps/\",\n",
        "    depth_dir=\"/content/drive/MyDrive/Capstone/SOBA-high-res/high_depth_maps/\",\n",
        "    transform=train_transform)\n",
        "\n",
        "image, shadow, depth = train_dataset[-1]\n",
        "print(image.shape)\n",
        "print(shadow.shape)\n",
        "print(depth.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q7R-HEacRVY",
        "outputId": "eefcca11-d515-403f-81ca-ec9c399712f8"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 256, 256])\n",
            "torch.Size([3, 256, 256])\n",
            "torch.Size([1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "mbU10pSMhkAc"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Experiments"
      ],
      "metadata": {
        "id": "-FUjzCt2c8fR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 - Attempt to use Huggingface Pipeline"
      ],
      "metadata": {
        "id": "qs26dcw4c_0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomStableDiffusionPipeline(StableDiffusionPipeline):\n",
        "    def __init__(self, unet, vae, text_encoder, tokenizer, scheduler, controlnet, safety_checker=None, feature_extractor=None):\n",
        "        super().__init__(unet=unet, vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, scheduler=scheduler, safety_checker=safety_checker, feature_extractor=feature_extractor)\n",
        "        self.controlnet = controlnet\n",
        "        self.image_head = nn.Conv2d(unet.config.in_channels, 3, kernel_size=1)\n",
        "        self.shadow_head = nn.Conv2d(unet.config.in_channels, 1, kernel_size=1)\n",
        "\n",
        "    def __call__(self, input_image, depth_map, prompt, height=None, width=None, **kwargs):\n",
        "        # Avoid error of height being ambiguous\n",
        "        if height is None:\n",
        "            height = self.unet.config.sample_size * self.vae_scale_factor\n",
        "        if width is None:\n",
        "            width = self.unet.config.sample_size * self.vae_scale_factor\n",
        "\n",
        "        # Resize depth map to fit the requirements of the pipeline\n",
        "        depth_map_resized = nn.functional.interpolate(depth_map, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
        "\n",
        "        text_inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(input_image.device)\n",
        "        text_encoder_output = self.text_encoder(**text_inputs)\n",
        "        encoder_hidden_states = text_encoder_output.last_hidden_state\n",
        "\n",
        "        # Timestep for scheduler\n",
        "        timestep = torch.tensor([1.0], dtype=torch.float32).to(input_image.device)\n",
        "\n",
        "        # Get conditioning input using ControlNet\n",
        "        controlnet_output = self.controlnet(\n",
        "            sample=depth_map_resized,\n",
        "            controlnet_cond=depth_map_resized,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            timestep=timestep\n",
        "        )\n",
        "\n",
        "        # Get diffusion model output with conditioning\n",
        "        unet_output = self.unet(\n",
        "            sample=input_image,\n",
        "            timestep=timestep,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            conditioning=controlnet_output.sample\n",
        "        ).sample\n",
        "\n",
        "        image_output = self.image_head(unet_output)\n",
        "        shadow_output = self.shadow_head(unet_output)\n",
        "\n",
        "        return image_output, shadow_output"
      ],
      "metadata": {
        "id": "4DY73q-tQlE6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample test of model\n",
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"tokenizer\")\n",
        "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_openpose\")\n",
        "scheduler = PNDMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
        "\n",
        "pipeline = CustomStableDiffusionPipeline(\n",
        "    unet=unet,\n",
        "    vae=vae,\n",
        "    text_encoder=text_encoder,\n",
        "    tokenizer=tokenizer,\n",
        "    scheduler=scheduler,\n",
        "    controlnet=controlnet\n",
        ")\n",
        "\n",
        "dummy_image = torch.randn(1, 3, 256, 256)\n",
        "dummy_depth = torch.randn(1, 1, 256, 256)\n",
        "dummy_prompt = \"a photo of a face\"\n",
        "\n",
        "image_output, shadow_output = pipeline(dummy_image, dummy_depth, dummy_prompt, height=256, width=256)\n",
        "print(image_output.shape, shadow_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "sg0bhyLtbxlJ",
        "outputId": "47ced694-e100-4919-cf1f-e91ab39f3f65"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
            "```\n",
            "pip install accelerate\n",
            "```\n",
            ".\n",
            "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
            "```\n",
            "pip install accelerate\n",
            "```\n",
            ".\n",
            "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
            "```\n",
            "pip install accelerate\n",
            "```\n",
            ".\n",
            "You have disabled the safety checker for <class '__main__.CustomStableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [320, 4, 3, 3], expected input[1, 1, 256, 256] to have 4 channels, but got 1 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-7f9fc2bca20a>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdummy_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"a photo of a face\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mimage_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-b073588b837a>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_image, depth_map, prompt, height, width, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Get conditioning input using ControlNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         controlnet_output = self.controlnet(\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth_map_resized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mcontrolnet_cond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth_map_resized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/controlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, controlnet_cond, conditioning_scale, class_labels, timestep_cond, attention_mask, added_cond_kwargs, cross_attention_kwargs, guess_mode, return_dict)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;31m# 2. pre-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mcontrolnet_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrolnet_cond_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrolnet_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [320, 4, 3, 3], expected input[1, 1, 256, 256] to have 4 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running into too many errors around input sizes - will continue to work on this using Huggingface documentation."
      ],
      "metadata": {
        "id": "7Wh8XDwUd6Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 - Simple Diffusion Model from Scratch"
      ],
      "metadata": {
        "id": "ItXuqLs6dfpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom UNet model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def down_block(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        def up_block(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.down1 = down_block(in_channels, 64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.down2 = down_block(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.down3 = down_block(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.down4 = down_block(256, 512)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.middle = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.up4 = up_block(1024, 512)\n",
        "        self.up3 = up_block(1024, 256)\n",
        "        self.up2 = up_block(512, 128)\n",
        "        self.up1 = up_block(256, 64)\n",
        "\n",
        "        self.output_image = nn.Conv2d(128, out_channels, kernel_size=1)\n",
        "        self.output_shadow = nn.Conv2d(128, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, depth):\n",
        "        # Concatenate depth map to input\n",
        "        x = torch.cat([x, depth], dim=1)\n",
        "\n",
        "        d1 = self.down1(x)\n",
        "        p1 = self.pool1(d1)\n",
        "        d2 = self.down2(p1)\n",
        "        p2 = self.pool2(d2)\n",
        "        d3 = self.down3(p2)\n",
        "        p3 = self.pool3(d3)\n",
        "        d4 = self.down4(p3)\n",
        "        p4 = self.pool4(d4)\n",
        "\n",
        "        middle = self.middle(p4)\n",
        "\n",
        "        u4 = self.up4(middle)\n",
        "        u4 = torch.cat([u4, d4], dim=1)\n",
        "        u3 = self.up3(u4)\n",
        "        u3 = torch.cat([u3, d3], dim=1)\n",
        "        u2 = self.up2(u3)\n",
        "        u2 = torch.cat([u2, d2], dim=1)\n",
        "        u1 = self.up1(u2)\n",
        "        u1 = torch.cat([u1, d1], dim=1)\n",
        "\n",
        "        image_output = self.output_image(u1)\n",
        "        shadow_output = self.output_shadow(u1)\n",
        "\n",
        "        return image_output, shadow_output"
      ],
      "metadata": {
        "id": "B2_TFqITdkem"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDiffusionModel(nn.Module):\n",
        "    def __init__(self, unet):\n",
        "        super(SimpleDiffusionModel, self).__init__()\n",
        "        self.unet = unet\n",
        "\n",
        "    def forward(self, x, depth, t):\n",
        "        return self.unet(x, depth)\n",
        "\n",
        "    def loss_fn(self, pred, target):\n",
        "        image_pred, shadow_pred = pred\n",
        "        image_target, shadow_target = target\n",
        "\n",
        "        image_loss = F.mse_loss(image_pred, image_target)\n",
        "        shadow_loss = F.mse_loss(shadow_pred, shadow_target)\n",
        "\n",
        "        return image_loss + shadow_loss"
      ],
      "metadata": {
        "id": "EjT_HYw9eIpD"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model architecture with dummy data\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return 100\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.randn(3, 256, 256)\n",
        "        shadow = torch.randn(3, 256, 256)\n",
        "        depth = torch.randn(1, 256, 256)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            shadow = self.transform(shadow)\n",
        "            depth = self.transform(depth)\n",
        "\n",
        "        return image, shadow, depth\n",
        "\n",
        "dummy_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "])\n",
        "\n",
        "dummy_dataset = DummyDataset(transform=dummy_transform)\n",
        "dataloader = DataLoader(dummy_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "Kl9h3XhGePHL"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "unet = UNet(in_channels=4, out_channels=3).to(device)\n",
        "model = SimpleDiffusionModel(unet=unet).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop for testing\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, shadows, depths in dataloader:\n",
        "        images, shadows, depths = images.to(device), shadows.to(device), depths.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, depths, t=0)\n",
        "        loss = model.loss_fn(outputs, (images, shadows))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1FG8Af2e5SB",
        "outputId": "a0fa07a4-2f3d-4fd6-b1b2-1e043ababbbf"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Loss: 2.4004\n",
            "Epoch [1/2], Loss: 2.3588\n",
            "Epoch [1/2], Loss: 2.3197\n",
            "Epoch [1/2], Loss: 2.2878\n",
            "Epoch [1/2], Loss: 2.2486\n",
            "Epoch [1/2], Loss: 2.2187\n",
            "Epoch [1/2], Loss: 2.1813\n",
            "Epoch [1/2], Loss: 2.1538\n",
            "Epoch [1/2], Loss: 2.1225\n",
            "Epoch [1/2], Loss: 2.0910\n",
            "Epoch [1/2], Loss: 2.0599\n",
            "Epoch [1/2], Loss: 2.0276\n",
            "Epoch [1/2], Loss: 2.0021\n",
            "Epoch [2/2], Loss: 1.9704\n",
            "Epoch [2/2], Loss: 1.9466\n",
            "Epoch [2/2], Loss: 1.9185\n",
            "Epoch [2/2], Loss: 1.8931\n",
            "Epoch [2/2], Loss: 1.8642\n",
            "Epoch [2/2], Loss: 1.8460\n",
            "Epoch [2/2], Loss: 1.8195\n",
            "Epoch [2/2], Loss: 1.7974\n",
            "Epoch [2/2], Loss: 1.7793\n",
            "Epoch [2/2], Loss: 1.7546\n",
            "Epoch [2/2], Loss: 1.7367\n",
            "Epoch [2/2], Loss: 1.7176\n",
            "Epoch [2/2], Loss: 1.6965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with Real Data"
      ],
      "metadata": {
        "id": "IM2L9rUzgZKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "unet = UNet(in_channels=4, out_channels=3).to(device)\n",
        "model = SimpleDiffusionModel(unet=unet).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, shadows, depths in train_dataloader:\n",
        "        images, shadows, depths = images.to(device), shadows.to(device), depths.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, depths, t=0)  # TODO: Experiment with different timesteps\n",
        "        loss = model.loss_fn(outputs, (images, shadows))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"simple_diffusion_model_epoch_{epoch+1}.pth\")"
      ],
      "metadata": {
        "id": "U8j9RvH7ufqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad39299-6790-4e48-fe4f-26d47443b281"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.7807\n",
            "Epoch [1/100], Loss: 0.5282\n",
            "Epoch [1/100], Loss: 0.4803\n",
            "Epoch [1/100], Loss: 0.4580\n",
            "Epoch [1/100], Loss: 0.4239\n",
            "Epoch [1/100], Loss: 0.3306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.2661\n",
            "Epoch [2/100], Loss: 0.3086\n",
            "Epoch [2/100], Loss: 0.3033\n",
            "Epoch [2/100], Loss: 0.3150\n",
            "Epoch [2/100], Loss: 0.2828\n",
            "Epoch [2/100], Loss: 0.2498\n",
            "Epoch [2/100], Loss: 0.2744\n",
            "Epoch [2/100], Loss: 0.2797\n",
            "Epoch [3/100], Loss: 0.2421\n",
            "Epoch [3/100], Loss: 0.2116\n",
            "Epoch [3/100], Loss: 0.1749\n",
            "Epoch [3/100], Loss: 0.2343\n",
            "Epoch [3/100], Loss: 0.1938\n",
            "Epoch [3/100], Loss: 0.2099\n",
            "Epoch [3/100], Loss: 0.1437\n",
            "Epoch [4/100], Loss: 0.1695\n",
            "Epoch [4/100], Loss: 0.1734\n",
            "Epoch [4/100], Loss: 0.1798\n",
            "Epoch [4/100], Loss: 0.2001\n",
            "Epoch [4/100], Loss: 0.1449\n",
            "Epoch [4/100], Loss: 0.1313\n",
            "Epoch [4/100], Loss: 0.1687\n",
            "Epoch [5/100], Loss: 0.1410\n",
            "Epoch [5/100], Loss: 0.1633\n",
            "Epoch [5/100], Loss: 0.1371\n",
            "Epoch [5/100], Loss: 0.1075\n",
            "Epoch [5/100], Loss: 0.1441\n",
            "Epoch [5/100], Loss: 0.1609\n",
            "Epoch [5/100], Loss: 0.1378\n",
            "Epoch [6/100], Loss: 0.1182\n",
            "Epoch [6/100], Loss: 0.1308\n",
            "Epoch [6/100], Loss: 0.1281\n",
            "Epoch [6/100], Loss: 0.1101\n",
            "Epoch [6/100], Loss: 0.1327\n",
            "Epoch [6/100], Loss: 0.1306\n",
            "Epoch [6/100], Loss: 0.2037\n",
            "Epoch [7/100], Loss: 0.1133\n",
            "Epoch [7/100], Loss: 0.1221\n",
            "Epoch [7/100], Loss: 0.1205\n",
            "Epoch [7/100], Loss: 0.1531\n",
            "Epoch [7/100], Loss: 0.1139\n",
            "Epoch [7/100], Loss: 0.1139\n",
            "Epoch [7/100], Loss: 0.0869\n",
            "Epoch [8/100], Loss: 0.1111\n",
            "Epoch [8/100], Loss: 0.1435\n",
            "Epoch [8/100], Loss: 0.1110\n",
            "Epoch [8/100], Loss: 0.0949\n",
            "Epoch [8/100], Loss: 0.1086\n",
            "Epoch [8/100], Loss: 0.1278\n",
            "Epoch [8/100], Loss: 0.1042\n",
            "Epoch [9/100], Loss: 0.1044\n",
            "Epoch [9/100], Loss: 0.0928\n",
            "Epoch [9/100], Loss: 0.1391\n",
            "Epoch [9/100], Loss: 0.1232\n",
            "Epoch [9/100], Loss: 0.0966\n",
            "Epoch [9/100], Loss: 0.1014\n",
            "Epoch [9/100], Loss: 0.1011\n",
            "Epoch [10/100], Loss: 0.1025\n",
            "Epoch [10/100], Loss: 0.0980\n",
            "Epoch [10/100], Loss: 0.0999\n",
            "Epoch [10/100], Loss: 0.1133\n",
            "Epoch [10/100], Loss: 0.1154\n",
            "Epoch [10/100], Loss: 0.1169\n",
            "Epoch [10/100], Loss: 0.0895\n",
            "Epoch [11/100], Loss: 0.1111\n",
            "Epoch [11/100], Loss: 0.1110\n",
            "Epoch [11/100], Loss: 0.0848\n",
            "Epoch [11/100], Loss: 0.1210\n",
            "Epoch [11/100], Loss: 0.1048\n",
            "Epoch [11/100], Loss: 0.0993\n",
            "Epoch [11/100], Loss: 0.1080\n",
            "Epoch [12/100], Loss: 0.1204\n",
            "Epoch [12/100], Loss: 0.1023\n",
            "Epoch [12/100], Loss: 0.1149\n",
            "Epoch [12/100], Loss: 0.1190\n",
            "Epoch [12/100], Loss: 0.1088\n",
            "Epoch [12/100], Loss: 0.0872\n",
            "Epoch [12/100], Loss: 0.0964\n",
            "Epoch [13/100], Loss: 0.1263\n",
            "Epoch [13/100], Loss: 0.0974\n",
            "Epoch [13/100], Loss: 0.0772\n",
            "Epoch [13/100], Loss: 0.0901\n",
            "Epoch [13/100], Loss: 0.1110\n",
            "Epoch [13/100], Loss: 0.1144\n",
            "Epoch [13/100], Loss: 0.1233\n",
            "Epoch [14/100], Loss: 0.1042\n",
            "Epoch [14/100], Loss: 0.1001\n",
            "Epoch [14/100], Loss: 0.1286\n",
            "Epoch [14/100], Loss: 0.0920\n",
            "Epoch [14/100], Loss: 0.1065\n",
            "Epoch [14/100], Loss: 0.1043\n",
            "Epoch [14/100], Loss: 0.0711\n",
            "Epoch [15/100], Loss: 0.0882\n",
            "Epoch [15/100], Loss: 0.1214\n",
            "Epoch [15/100], Loss: 0.0977\n",
            "Epoch [15/100], Loss: 0.1109\n",
            "Epoch [15/100], Loss: 0.0947\n",
            "Epoch [15/100], Loss: 0.0812\n",
            "Epoch [15/100], Loss: 0.1675\n",
            "Epoch [16/100], Loss: 0.0752\n",
            "Epoch [16/100], Loss: 0.1085\n",
            "Epoch [16/100], Loss: 0.0955\n",
            "Epoch [16/100], Loss: 0.1339\n",
            "Epoch [16/100], Loss: 0.0786\n",
            "Epoch [16/100], Loss: 0.1105\n",
            "Epoch [16/100], Loss: 0.0871\n",
            "Epoch [17/100], Loss: 0.1023\n",
            "Epoch [17/100], Loss: 0.0938\n",
            "Epoch [17/100], Loss: 0.0892\n",
            "Epoch [17/100], Loss: 0.1207\n",
            "Epoch [17/100], Loss: 0.0744\n",
            "Epoch [17/100], Loss: 0.1133\n",
            "Epoch [17/100], Loss: 0.0782\n",
            "Epoch [18/100], Loss: 0.0877\n",
            "Epoch [18/100], Loss: 0.1206\n",
            "Epoch [18/100], Loss: 0.1022\n",
            "Epoch [18/100], Loss: 0.0845\n",
            "Epoch [18/100], Loss: 0.0970\n",
            "Epoch [18/100], Loss: 0.0876\n",
            "Epoch [18/100], Loss: 0.1449\n",
            "Epoch [19/100], Loss: 0.0691\n",
            "Epoch [19/100], Loss: 0.1049\n",
            "Epoch [19/100], Loss: 0.0676\n",
            "Epoch [19/100], Loss: 0.1342\n",
            "Epoch [19/100], Loss: 0.1194\n",
            "Epoch [19/100], Loss: 0.0796\n",
            "Epoch [19/100], Loss: 0.1287\n",
            "Epoch [20/100], Loss: 0.0950\n",
            "Epoch [20/100], Loss: 0.0876\n",
            "Epoch [20/100], Loss: 0.1152\n",
            "Epoch [20/100], Loss: 0.0990\n",
            "Epoch [20/100], Loss: 0.0827\n",
            "Epoch [20/100], Loss: 0.0853\n",
            "Epoch [20/100], Loss: 0.0655\n",
            "Epoch [21/100], Loss: 0.0705\n",
            "Epoch [21/100], Loss: 0.0928\n",
            "Epoch [21/100], Loss: 0.0960\n",
            "Epoch [21/100], Loss: 0.0896\n",
            "Epoch [21/100], Loss: 0.1130\n",
            "Epoch [21/100], Loss: 0.1006\n",
            "Epoch [21/100], Loss: 0.1188\n",
            "Epoch [22/100], Loss: 0.0805\n",
            "Epoch [22/100], Loss: 0.0939\n",
            "Epoch [22/100], Loss: 0.0875\n",
            "Epoch [22/100], Loss: 0.0971\n",
            "Epoch [22/100], Loss: 0.1086\n",
            "Epoch [22/100], Loss: 0.0905\n",
            "Epoch [22/100], Loss: 0.0732\n",
            "Epoch [23/100], Loss: 0.1073\n",
            "Epoch [23/100], Loss: 0.0697\n",
            "Epoch [23/100], Loss: 0.1103\n",
            "Epoch [23/100], Loss: 0.0698\n",
            "Epoch [23/100], Loss: 0.1042\n",
            "Epoch [23/100], Loss: 0.1121\n",
            "Epoch [23/100], Loss: 0.0610\n",
            "Epoch [24/100], Loss: 0.0859\n",
            "Epoch [24/100], Loss: 0.0969\n",
            "Epoch [24/100], Loss: 0.0811\n",
            "Epoch [24/100], Loss: 0.0865\n",
            "Epoch [24/100], Loss: 0.1188\n",
            "Epoch [24/100], Loss: 0.0971\n",
            "Epoch [24/100], Loss: 0.0860\n",
            "Epoch [25/100], Loss: 0.0771\n",
            "Epoch [25/100], Loss: 0.0736\n",
            "Epoch [25/100], Loss: 0.1123\n",
            "Epoch [25/100], Loss: 0.0905\n",
            "Epoch [25/100], Loss: 0.0809\n",
            "Epoch [25/100], Loss: 0.1101\n",
            "Epoch [25/100], Loss: 0.0924\n",
            "Epoch [26/100], Loss: 0.0787\n",
            "Epoch [26/100], Loss: 0.0741\n",
            "Epoch [26/100], Loss: 0.0899\n",
            "Epoch [26/100], Loss: 0.0955\n",
            "Epoch [26/100], Loss: 0.0732\n",
            "Epoch [26/100], Loss: 0.1239\n",
            "Epoch [26/100], Loss: 0.1478\n",
            "Epoch [27/100], Loss: 0.0848\n",
            "Epoch [27/100], Loss: 0.1126\n",
            "Epoch [27/100], Loss: 0.0786\n",
            "Epoch [27/100], Loss: 0.0832\n",
            "Epoch [27/100], Loss: 0.1203\n",
            "Epoch [27/100], Loss: 0.0912\n",
            "Epoch [27/100], Loss: 0.0507\n",
            "Epoch [28/100], Loss: 0.0858\n",
            "Epoch [28/100], Loss: 0.1011\n",
            "Epoch [28/100], Loss: 0.0835\n",
            "Epoch [28/100], Loss: 0.0979\n",
            "Epoch [28/100], Loss: 0.0892\n",
            "Epoch [28/100], Loss: 0.0735\n",
            "Epoch [28/100], Loss: 0.1681\n",
            "Epoch [29/100], Loss: 0.0961\n",
            "Epoch [29/100], Loss: 0.0886\n",
            "Epoch [29/100], Loss: 0.0943\n",
            "Epoch [29/100], Loss: 0.0897\n",
            "Epoch [29/100], Loss: 0.0726\n",
            "Epoch [29/100], Loss: 0.0913\n",
            "Epoch [29/100], Loss: 0.0852\n",
            "Epoch [30/100], Loss: 0.1042\n",
            "Epoch [30/100], Loss: 0.1120\n",
            "Epoch [30/100], Loss: 0.0945\n",
            "Epoch [30/100], Loss: 0.0709\n",
            "Epoch [30/100], Loss: 0.0748\n",
            "Epoch [30/100], Loss: 0.0628\n",
            "Epoch [30/100], Loss: 0.0742\n",
            "Epoch [31/100], Loss: 0.0671\n",
            "Epoch [31/100], Loss: 0.0830\n",
            "Epoch [31/100], Loss: 0.0895\n",
            "Epoch [31/100], Loss: 0.1000\n",
            "Epoch [31/100], Loss: 0.0934\n",
            "Epoch [31/100], Loss: 0.0890\n",
            "Epoch [31/100], Loss: 0.1577\n",
            "Epoch [32/100], Loss: 0.1028\n",
            "Epoch [32/100], Loss: 0.0659\n",
            "Epoch [32/100], Loss: 0.1242\n",
            "Epoch [32/100], Loss: 0.0857\n",
            "Epoch [32/100], Loss: 0.0999\n",
            "Epoch [32/100], Loss: 0.0655\n",
            "Epoch [32/100], Loss: 0.0797\n",
            "Epoch [33/100], Loss: 0.0712\n",
            "Epoch [33/100], Loss: 0.0882\n",
            "Epoch [33/100], Loss: 0.0899\n",
            "Epoch [33/100], Loss: 0.0832\n",
            "Epoch [33/100], Loss: 0.0746\n",
            "Epoch [33/100], Loss: 0.1335\n",
            "Epoch [33/100], Loss: 0.0687\n",
            "Epoch [34/100], Loss: 0.0759\n",
            "Epoch [34/100], Loss: 0.1148\n",
            "Epoch [34/100], Loss: 0.0760\n",
            "Epoch [34/100], Loss: 0.0918\n",
            "Epoch [34/100], Loss: 0.0753\n",
            "Epoch [34/100], Loss: 0.0792\n",
            "Epoch [34/100], Loss: 0.1187\n",
            "Epoch [35/100], Loss: 0.0885\n",
            "Epoch [35/100], Loss: 0.0967\n",
            "Epoch [35/100], Loss: 0.0919\n",
            "Epoch [35/100], Loss: 0.0935\n",
            "Epoch [35/100], Loss: 0.0722\n",
            "Epoch [35/100], Loss: 0.0615\n",
            "Epoch [35/100], Loss: 0.1055\n",
            "Epoch [36/100], Loss: 0.0723\n",
            "Epoch [36/100], Loss: 0.0980\n",
            "Epoch [36/100], Loss: 0.0696\n",
            "Epoch [36/100], Loss: 0.1079\n",
            "Epoch [36/100], Loss: 0.0775\n",
            "Epoch [36/100], Loss: 0.0791\n",
            "Epoch [36/100], Loss: 0.1108\n",
            "Epoch [37/100], Loss: 0.0895\n",
            "Epoch [37/100], Loss: 0.0789\n",
            "Epoch [37/100], Loss: 0.0848\n",
            "Epoch [37/100], Loss: 0.0938\n",
            "Epoch [37/100], Loss: 0.0705\n",
            "Epoch [37/100], Loss: 0.0762\n",
            "Epoch [37/100], Loss: 0.1225\n",
            "Epoch [38/100], Loss: 0.0792\n",
            "Epoch [38/100], Loss: 0.1042\n",
            "Epoch [38/100], Loss: 0.0815\n",
            "Epoch [38/100], Loss: 0.0575\n",
            "Epoch [38/100], Loss: 0.1078\n",
            "Epoch [38/100], Loss: 0.0866\n",
            "Epoch [38/100], Loss: 0.0860\n",
            "Epoch [39/100], Loss: 0.0651\n",
            "Epoch [39/100], Loss: 0.1166\n",
            "Epoch [39/100], Loss: 0.0665\n",
            "Epoch [39/100], Loss: 0.0844\n",
            "Epoch [39/100], Loss: 0.0869\n",
            "Epoch [39/100], Loss: 0.0926\n",
            "Epoch [39/100], Loss: 0.0713\n",
            "Epoch [40/100], Loss: 0.0755\n",
            "Epoch [40/100], Loss: 0.0992\n",
            "Epoch [40/100], Loss: 0.1022\n",
            "Epoch [40/100], Loss: 0.0771\n",
            "Epoch [40/100], Loss: 0.0687\n",
            "Epoch [40/100], Loss: 0.0774\n",
            "Epoch [40/100], Loss: 0.0904\n",
            "Epoch [41/100], Loss: 0.0702\n",
            "Epoch [41/100], Loss: 0.1113\n",
            "Epoch [41/100], Loss: 0.0952\n",
            "Epoch [41/100], Loss: 0.0534\n",
            "Epoch [41/100], Loss: 0.0781\n",
            "Epoch [41/100], Loss: 0.0683\n",
            "Epoch [41/100], Loss: 0.1390\n",
            "Epoch [42/100], Loss: 0.0912\n",
            "Epoch [42/100], Loss: 0.0626\n",
            "Epoch [42/100], Loss: 0.0910\n",
            "Epoch [42/100], Loss: 0.0979\n",
            "Epoch [42/100], Loss: 0.0806\n",
            "Epoch [42/100], Loss: 0.0617\n",
            "Epoch [42/100], Loss: 0.0866\n",
            "Epoch [43/100], Loss: 0.0790\n",
            "Epoch [43/100], Loss: 0.0949\n",
            "Epoch [43/100], Loss: 0.0887\n",
            "Epoch [43/100], Loss: 0.0885\n",
            "Epoch [43/100], Loss: 0.0810\n",
            "Epoch [43/100], Loss: 0.0549\n",
            "Epoch [43/100], Loss: 0.0443\n",
            "Epoch [44/100], Loss: 0.0921\n",
            "Epoch [44/100], Loss: 0.0579\n",
            "Epoch [44/100], Loss: 0.1055\n",
            "Epoch [44/100], Loss: 0.0825\n",
            "Epoch [44/100], Loss: 0.0853\n",
            "Epoch [44/100], Loss: 0.0684\n",
            "Epoch [44/100], Loss: 0.0542\n",
            "Epoch [45/100], Loss: 0.0743\n",
            "Epoch [45/100], Loss: 0.0753\n",
            "Epoch [45/100], Loss: 0.0973\n",
            "Epoch [45/100], Loss: 0.0708\n",
            "Epoch [45/100], Loss: 0.0530\n",
            "Epoch [45/100], Loss: 0.1045\n",
            "Epoch [45/100], Loss: 0.0579\n",
            "Epoch [46/100], Loss: 0.0663\n",
            "Epoch [46/100], Loss: 0.0962\n",
            "Epoch [46/100], Loss: 0.0659\n",
            "Epoch [46/100], Loss: 0.0595\n",
            "Epoch [46/100], Loss: 0.0949\n",
            "Epoch [46/100], Loss: 0.0796\n",
            "Epoch [46/100], Loss: 0.0881\n",
            "Epoch [47/100], Loss: 0.0724\n",
            "Epoch [47/100], Loss: 0.0792\n",
            "Epoch [47/100], Loss: 0.0956\n",
            "Epoch [47/100], Loss: 0.0829\n",
            "Epoch [47/100], Loss: 0.0609\n",
            "Epoch [47/100], Loss: 0.0845\n",
            "Epoch [47/100], Loss: 0.0641\n",
            "Epoch [48/100], Loss: 0.0711\n",
            "Epoch [48/100], Loss: 0.1000\n",
            "Epoch [48/100], Loss: 0.0960\n",
            "Epoch [48/100], Loss: 0.0673\n",
            "Epoch [48/100], Loss: 0.0715\n",
            "Epoch [48/100], Loss: 0.0695\n",
            "Epoch [48/100], Loss: 0.0647\n",
            "Epoch [49/100], Loss: 0.0839\n",
            "Epoch [49/100], Loss: 0.0573\n",
            "Epoch [49/100], Loss: 0.0849\n",
            "Epoch [49/100], Loss: 0.0636\n",
            "Epoch [49/100], Loss: 0.1003\n",
            "Epoch [49/100], Loss: 0.0830\n",
            "Epoch [49/100], Loss: 0.0685\n",
            "Epoch [50/100], Loss: 0.0968\n",
            "Epoch [50/100], Loss: 0.0788\n",
            "Epoch [50/100], Loss: 0.0838\n",
            "Epoch [50/100], Loss: 0.0605\n",
            "Epoch [50/100], Loss: 0.0783\n",
            "Epoch [50/100], Loss: 0.0580\n",
            "Epoch [50/100], Loss: 0.0806\n",
            "Epoch [51/100], Loss: 0.0661\n",
            "Epoch [51/100], Loss: 0.1001\n",
            "Epoch [51/100], Loss: 0.0741\n",
            "Epoch [51/100], Loss: 0.0736\n",
            "Epoch [51/100], Loss: 0.0641\n",
            "Epoch [51/100], Loss: 0.0795\n",
            "Epoch [51/100], Loss: 0.1141\n",
            "Epoch [52/100], Loss: 0.0809\n",
            "Epoch [52/100], Loss: 0.0679\n",
            "Epoch [52/100], Loss: 0.0863\n",
            "Epoch [52/100], Loss: 0.0879\n",
            "Epoch [52/100], Loss: 0.0700\n",
            "Epoch [52/100], Loss: 0.0721\n",
            "Epoch [52/100], Loss: 0.0665\n",
            "Epoch [53/100], Loss: 0.0590\n",
            "Epoch [53/100], Loss: 0.0788\n",
            "Epoch [53/100], Loss: 0.1051\n",
            "Epoch [53/100], Loss: 0.0596\n",
            "Epoch [53/100], Loss: 0.0911\n",
            "Epoch [53/100], Loss: 0.0695\n",
            "Epoch [53/100], Loss: 0.0631\n",
            "Epoch [54/100], Loss: 0.0911\n",
            "Epoch [54/100], Loss: 0.0551\n",
            "Epoch [54/100], Loss: 0.0587\n",
            "Epoch [54/100], Loss: 0.0898\n",
            "Epoch [54/100], Loss: 0.0953\n",
            "Epoch [54/100], Loss: 0.0679\n",
            "Epoch [54/100], Loss: 0.0980\n",
            "Epoch [55/100], Loss: 0.0683\n",
            "Epoch [55/100], Loss: 0.0780\n",
            "Epoch [55/100], Loss: 0.0742\n",
            "Epoch [55/100], Loss: 0.0842\n",
            "Epoch [55/100], Loss: 0.1105\n",
            "Epoch [55/100], Loss: 0.0591\n",
            "Epoch [55/100], Loss: 0.0817\n",
            "Epoch [56/100], Loss: 0.0560\n",
            "Epoch [56/100], Loss: 0.0770\n",
            "Epoch [56/100], Loss: 0.0664\n",
            "Epoch [56/100], Loss: 0.0688\n",
            "Epoch [56/100], Loss: 0.0846\n",
            "Epoch [56/100], Loss: 0.1096\n",
            "Epoch [56/100], Loss: 0.0781\n",
            "Epoch [57/100], Loss: 0.0542\n",
            "Epoch [57/100], Loss: 0.0965\n",
            "Epoch [57/100], Loss: 0.0768\n",
            "Epoch [57/100], Loss: 0.0688\n",
            "Epoch [57/100], Loss: 0.0692\n",
            "Epoch [57/100], Loss: 0.0840\n",
            "Epoch [57/100], Loss: 0.0557\n",
            "Epoch [58/100], Loss: 0.0662\n",
            "Epoch [58/100], Loss: 0.0955\n",
            "Epoch [58/100], Loss: 0.0535\n",
            "Epoch [58/100], Loss: 0.0664\n",
            "Epoch [58/100], Loss: 0.0907\n",
            "Epoch [58/100], Loss: 0.0684\n",
            "Epoch [58/100], Loss: 0.0810\n",
            "Epoch [59/100], Loss: 0.0646\n",
            "Epoch [59/100], Loss: 0.0732\n",
            "Epoch [59/100], Loss: 0.0910\n",
            "Epoch [59/100], Loss: 0.0786\n",
            "Epoch [59/100], Loss: 0.0674\n",
            "Epoch [59/100], Loss: 0.0592\n",
            "Epoch [59/100], Loss: 0.0810\n",
            "Epoch [60/100], Loss: 0.0883\n",
            "Epoch [60/100], Loss: 0.0706\n",
            "Epoch [60/100], Loss: 0.0526\n",
            "Epoch [60/100], Loss: 0.0816\n",
            "Epoch [60/100], Loss: 0.0624\n",
            "Epoch [60/100], Loss: 0.0757\n",
            "Epoch [60/100], Loss: 0.0644\n",
            "Epoch [61/100], Loss: 0.0778\n",
            "Epoch [61/100], Loss: 0.0565\n",
            "Epoch [61/100], Loss: 0.0779\n",
            "Epoch [61/100], Loss: 0.0816\n",
            "Epoch [61/100], Loss: 0.0767\n",
            "Epoch [61/100], Loss: 0.0593\n",
            "Epoch [61/100], Loss: 0.0662\n",
            "Epoch [62/100], Loss: 0.0620\n",
            "Epoch [62/100], Loss: 0.0651\n",
            "Epoch [62/100], Loss: 0.0771\n",
            "Epoch [62/100], Loss: 0.0754\n",
            "Epoch [62/100], Loss: 0.0639\n",
            "Epoch [62/100], Loss: 0.0841\n",
            "Epoch [62/100], Loss: 0.0489\n",
            "Epoch [63/100], Loss: 0.0711\n",
            "Epoch [63/100], Loss: 0.0758\n",
            "Epoch [63/100], Loss: 0.0643\n",
            "Epoch [63/100], Loss: 0.0587\n",
            "Epoch [63/100], Loss: 0.0957\n",
            "Epoch [63/100], Loss: 0.0432\n",
            "Epoch [63/100], Loss: 0.0906\n",
            "Epoch [64/100], Loss: 0.0738\n",
            "Epoch [64/100], Loss: 0.0728\n",
            "Epoch [64/100], Loss: 0.0599\n",
            "Epoch [64/100], Loss: 0.0645\n",
            "Epoch [64/100], Loss: 0.0670\n",
            "Epoch [64/100], Loss: 0.0674\n",
            "Epoch [64/100], Loss: 0.0605\n",
            "Epoch [65/100], Loss: 0.0662\n",
            "Epoch [65/100], Loss: 0.0717\n",
            "Epoch [65/100], Loss: 0.0716\n",
            "Epoch [65/100], Loss: 0.0666\n",
            "Epoch [65/100], Loss: 0.0727\n",
            "Epoch [65/100], Loss: 0.0610\n",
            "Epoch [65/100], Loss: 0.0673\n",
            "Epoch [66/100], Loss: 0.0726\n",
            "Epoch [66/100], Loss: 0.0560\n",
            "Epoch [66/100], Loss: 0.0549\n",
            "Epoch [66/100], Loss: 0.0541\n",
            "Epoch [66/100], Loss: 0.0825\n",
            "Epoch [66/100], Loss: 0.0645\n",
            "Epoch [66/100], Loss: 0.0958\n",
            "Epoch [67/100], Loss: 0.0583\n",
            "Epoch [67/100], Loss: 0.0722\n",
            "Epoch [67/100], Loss: 0.0554\n",
            "Epoch [67/100], Loss: 0.0750\n",
            "Epoch [67/100], Loss: 0.0635\n",
            "Epoch [67/100], Loss: 0.0586\n",
            "Epoch [67/100], Loss: 0.1251\n",
            "Epoch [68/100], Loss: 0.0757\n",
            "Epoch [68/100], Loss: 0.0523\n",
            "Epoch [68/100], Loss: 0.0524\n",
            "Epoch [68/100], Loss: 0.0808\n",
            "Epoch [68/100], Loss: 0.0696\n",
            "Epoch [68/100], Loss: 0.0560\n",
            "Epoch [68/100], Loss: 0.0584\n",
            "Epoch [69/100], Loss: 0.0777\n",
            "Epoch [69/100], Loss: 0.0514\n",
            "Epoch [69/100], Loss: 0.0754\n",
            "Epoch [69/100], Loss: 0.0591\n",
            "Epoch [69/100], Loss: 0.0516\n",
            "Epoch [69/100], Loss: 0.0568\n",
            "Epoch [69/100], Loss: 0.1315\n",
            "Epoch [70/100], Loss: 0.0795\n",
            "Epoch [70/100], Loss: 0.0583\n",
            "Epoch [70/100], Loss: 0.0522\n",
            "Epoch [70/100], Loss: 0.0550\n",
            "Epoch [70/100], Loss: 0.0410\n",
            "Epoch [70/100], Loss: 0.0881\n",
            "Epoch [70/100], Loss: 0.0966\n",
            "Epoch [71/100], Loss: 0.0573\n",
            "Epoch [71/100], Loss: 0.0616\n",
            "Epoch [71/100], Loss: 0.0614\n",
            "Epoch [71/100], Loss: 0.0841\n",
            "Epoch [71/100], Loss: 0.0725\n",
            "Epoch [71/100], Loss: 0.0631\n",
            "Epoch [71/100], Loss: 0.0930\n",
            "Epoch [72/100], Loss: 0.0583\n",
            "Epoch [72/100], Loss: 0.0705\n",
            "Epoch [72/100], Loss: 0.0831\n",
            "Epoch [72/100], Loss: 0.0503\n",
            "Epoch [72/100], Loss: 0.0691\n",
            "Epoch [72/100], Loss: 0.0593\n",
            "Epoch [72/100], Loss: 0.0661\n",
            "Epoch [73/100], Loss: 0.0545\n",
            "Epoch [73/100], Loss: 0.0700\n",
            "Epoch [73/100], Loss: 0.0583\n",
            "Epoch [73/100], Loss: 0.0681\n",
            "Epoch [73/100], Loss: 0.0863\n",
            "Epoch [73/100], Loss: 0.0476\n",
            "Epoch [73/100], Loss: 0.0692\n",
            "Epoch [74/100], Loss: 0.0628\n",
            "Epoch [74/100], Loss: 0.0840\n",
            "Epoch [74/100], Loss: 0.0709\n",
            "Epoch [74/100], Loss: 0.0605\n",
            "Epoch [74/100], Loss: 0.0477\n",
            "Epoch [74/100], Loss: 0.0631\n",
            "Epoch [74/100], Loss: 0.0972\n",
            "Epoch [75/100], Loss: 0.0526\n",
            "Epoch [75/100], Loss: 0.0742\n",
            "Epoch [75/100], Loss: 0.0677\n",
            "Epoch [75/100], Loss: 0.0819\n",
            "Epoch [75/100], Loss: 0.0491\n",
            "Epoch [75/100], Loss: 0.0550\n",
            "Epoch [75/100], Loss: 0.0768\n",
            "Epoch [76/100], Loss: 0.0438\n",
            "Epoch [76/100], Loss: 0.0699\n",
            "Epoch [76/100], Loss: 0.0721\n",
            "Epoch [76/100], Loss: 0.0630\n",
            "Epoch [76/100], Loss: 0.0709\n",
            "Epoch [76/100], Loss: 0.0535\n",
            "Epoch [76/100], Loss: 0.0527\n",
            "Epoch [77/100], Loss: 0.0663\n",
            "Epoch [77/100], Loss: 0.0603\n",
            "Epoch [77/100], Loss: 0.0516\n",
            "Epoch [77/100], Loss: 0.0684\n",
            "Epoch [77/100], Loss: 0.0521\n",
            "Epoch [77/100], Loss: 0.0565\n",
            "Epoch [77/100], Loss: 0.0476\n",
            "Epoch [78/100], Loss: 0.0581\n",
            "Epoch [78/100], Loss: 0.0640\n",
            "Epoch [78/100], Loss: 0.0564\n",
            "Epoch [78/100], Loss: 0.0742\n",
            "Epoch [78/100], Loss: 0.0514\n",
            "Epoch [78/100], Loss: 0.0521\n",
            "Epoch [78/100], Loss: 0.0697\n",
            "Epoch [79/100], Loss: 0.0441\n",
            "Epoch [79/100], Loss: 0.0545\n",
            "Epoch [79/100], Loss: 0.0602\n",
            "Epoch [79/100], Loss: 0.0511\n",
            "Epoch [79/100], Loss: 0.0718\n",
            "Epoch [79/100], Loss: 0.0612\n",
            "Epoch [79/100], Loss: 0.0530\n",
            "Epoch [80/100], Loss: 0.0607\n",
            "Epoch [80/100], Loss: 0.0420\n",
            "Epoch [80/100], Loss: 0.0544\n",
            "Epoch [80/100], Loss: 0.0560\n",
            "Epoch [80/100], Loss: 0.0711\n",
            "Epoch [80/100], Loss: 0.0591\n",
            "Epoch [80/100], Loss: 0.0494\n",
            "Epoch [81/100], Loss: 0.0448\n",
            "Epoch [81/100], Loss: 0.0719\n",
            "Epoch [81/100], Loss: 0.0633\n",
            "Epoch [81/100], Loss: 0.0611\n",
            "Epoch [81/100], Loss: 0.0402\n",
            "Epoch [81/100], Loss: 0.0620\n",
            "Epoch [81/100], Loss: 0.0402\n",
            "Epoch [82/100], Loss: 0.0594\n",
            "Epoch [82/100], Loss: 0.0575\n",
            "Epoch [82/100], Loss: 0.0521\n",
            "Epoch [82/100], Loss: 0.0628\n",
            "Epoch [82/100], Loss: 0.0563\n",
            "Epoch [82/100], Loss: 0.0550\n",
            "Epoch [82/100], Loss: 0.0572\n",
            "Epoch [83/100], Loss: 0.0498\n",
            "Epoch [83/100], Loss: 0.0481\n",
            "Epoch [83/100], Loss: 0.0376\n",
            "Epoch [83/100], Loss: 0.0528\n",
            "Epoch [83/100], Loss: 0.0615\n",
            "Epoch [83/100], Loss: 0.0753\n",
            "Epoch [83/100], Loss: 0.0937\n",
            "Epoch [84/100], Loss: 0.0518\n",
            "Epoch [84/100], Loss: 0.0492\n",
            "Epoch [84/100], Loss: 0.0471\n",
            "Epoch [84/100], Loss: 0.0599\n",
            "Epoch [84/100], Loss: 0.0498\n",
            "Epoch [84/100], Loss: 0.0697\n",
            "Epoch [84/100], Loss: 0.0410\n",
            "Epoch [85/100], Loss: 0.0698\n",
            "Epoch [85/100], Loss: 0.0555\n",
            "Epoch [85/100], Loss: 0.0469\n",
            "Epoch [85/100], Loss: 0.0513\n",
            "Epoch [85/100], Loss: 0.0578\n",
            "Epoch [85/100], Loss: 0.0532\n",
            "Epoch [85/100], Loss: 0.0491\n",
            "Epoch [86/100], Loss: 0.0545\n",
            "Epoch [86/100], Loss: 0.0367\n",
            "Epoch [86/100], Loss: 0.0494\n",
            "Epoch [86/100], Loss: 0.0463\n",
            "Epoch [86/100], Loss: 0.0564\n",
            "Epoch [86/100], Loss: 0.0619\n",
            "Epoch [86/100], Loss: 0.1557\n",
            "Epoch [87/100], Loss: 0.0447\n",
            "Epoch [87/100], Loss: 0.0640\n",
            "Epoch [87/100], Loss: 0.0505\n",
            "Epoch [87/100], Loss: 0.0474\n",
            "Epoch [87/100], Loss: 0.0623\n",
            "Epoch [87/100], Loss: 0.0653\n",
            "Epoch [87/100], Loss: 0.0380\n",
            "Epoch [88/100], Loss: 0.0525\n",
            "Epoch [88/100], Loss: 0.0591\n",
            "Epoch [88/100], Loss: 0.0551\n",
            "Epoch [88/100], Loss: 0.0417\n",
            "Epoch [88/100], Loss: 0.0434\n",
            "Epoch [88/100], Loss: 0.0613\n",
            "Epoch [88/100], Loss: 0.0906\n",
            "Epoch [89/100], Loss: 0.0517\n",
            "Epoch [89/100], Loss: 0.0453\n",
            "Epoch [89/100], Loss: 0.0476\n",
            "Epoch [89/100], Loss: 0.0577\n",
            "Epoch [89/100], Loss: 0.0603\n",
            "Epoch [89/100], Loss: 0.0413\n",
            "Epoch [89/100], Loss: 0.0630\n",
            "Epoch [90/100], Loss: 0.0381\n",
            "Epoch [90/100], Loss: 0.0524\n",
            "Epoch [90/100], Loss: 0.0459\n",
            "Epoch [90/100], Loss: 0.0395\n",
            "Epoch [90/100], Loss: 0.0531\n",
            "Epoch [90/100], Loss: 0.0559\n",
            "Epoch [90/100], Loss: 0.1110\n",
            "Epoch [91/100], Loss: 0.0420\n",
            "Epoch [91/100], Loss: 0.0630\n",
            "Epoch [91/100], Loss: 0.0363\n",
            "Epoch [91/100], Loss: 0.0458\n",
            "Epoch [91/100], Loss: 0.0435\n",
            "Epoch [91/100], Loss: 0.0630\n",
            "Epoch [91/100], Loss: 0.0532\n",
            "Epoch [92/100], Loss: 0.0329\n",
            "Epoch [92/100], Loss: 0.0429\n",
            "Epoch [92/100], Loss: 0.0568\n",
            "Epoch [92/100], Loss: 0.0518\n",
            "Epoch [92/100], Loss: 0.0549\n",
            "Epoch [92/100], Loss: 0.0581\n",
            "Epoch [92/100], Loss: 0.0385\n",
            "Epoch [93/100], Loss: 0.0407\n",
            "Epoch [93/100], Loss: 0.0602\n",
            "Epoch [93/100], Loss: 0.0434\n",
            "Epoch [93/100], Loss: 0.0439\n",
            "Epoch [93/100], Loss: 0.0476\n",
            "Epoch [93/100], Loss: 0.0473\n",
            "Epoch [93/100], Loss: 0.0471\n",
            "Epoch [94/100], Loss: 0.0360\n",
            "Epoch [94/100], Loss: 0.0597\n",
            "Epoch [94/100], Loss: 0.0373\n",
            "Epoch [94/100], Loss: 0.0547\n",
            "Epoch [94/100], Loss: 0.0322\n",
            "Epoch [94/100], Loss: 0.0525\n",
            "Epoch [94/100], Loss: 0.0328\n",
            "Epoch [95/100], Loss: 0.0369\n",
            "Epoch [95/100], Loss: 0.0550\n",
            "Epoch [95/100], Loss: 0.0364\n",
            "Epoch [95/100], Loss: 0.0469\n",
            "Epoch [95/100], Loss: 0.0418\n",
            "Epoch [95/100], Loss: 0.0555\n",
            "Epoch [95/100], Loss: 0.0449\n",
            "Epoch [96/100], Loss: 0.0395\n",
            "Epoch [96/100], Loss: 0.0409\n",
            "Epoch [96/100], Loss: 0.0459\n",
            "Epoch [96/100], Loss: 0.0349\n",
            "Epoch [96/100], Loss: 0.0564\n",
            "Epoch [96/100], Loss: 0.0420\n",
            "Epoch [96/100], Loss: 0.0372\n",
            "Epoch [97/100], Loss: 0.0385\n",
            "Epoch [97/100], Loss: 0.0486\n",
            "Epoch [97/100], Loss: 0.0463\n",
            "Epoch [97/100], Loss: 0.0383\n",
            "Epoch [97/100], Loss: 0.0402\n",
            "Epoch [97/100], Loss: 0.0433\n",
            "Epoch [97/100], Loss: 0.0831\n",
            "Epoch [98/100], Loss: 0.0461\n",
            "Epoch [98/100], Loss: 0.0525\n",
            "Epoch [98/100], Loss: 0.0425\n",
            "Epoch [98/100], Loss: 0.0438\n",
            "Epoch [98/100], Loss: 0.0383\n",
            "Epoch [98/100], Loss: 0.0333\n",
            "Epoch [98/100], Loss: 0.0669\n",
            "Epoch [99/100], Loss: 0.0441\n",
            "Epoch [99/100], Loss: 0.0476\n",
            "Epoch [99/100], Loss: 0.0486\n",
            "Epoch [99/100], Loss: 0.0391\n",
            "Epoch [99/100], Loss: 0.0466\n",
            "Epoch [99/100], Loss: 0.0380\n",
            "Epoch [99/100], Loss: 0.0260\n",
            "Epoch [100/100], Loss: 0.0331\n",
            "Epoch [100/100], Loss: 0.0370\n",
            "Epoch [100/100], Loss: 0.0499\n",
            "Epoch [100/100], Loss: 0.0505\n",
            "Epoch [100/100], Loss: 0.0415\n",
            "Epoch [100/100], Loss: 0.0300\n",
            "Epoch [100/100], Loss: 0.0473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Continue work with pre-trained models for better generation"
      ],
      "metadata": {
        "id": "gMsQoVl0k3Y_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}